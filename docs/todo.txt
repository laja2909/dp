# test if the initialisation still works

# airflow
# postgres database

# There should be only one config file, but now there is many... terraform variable file and confs.json need to do something about it.

# Add pytest logic
# maybe add api class since many apiclasses now uses similar logic.











#Problem:
--now that we have workflow in main branch. What about if we want to setup all resources again. If we now follow up the setup we need to merge our dev to main and then workflow stars which will fail!
----Need to do some logic so that all terraform stuff needs to be loaded first before running our workflow.
----Also in terraform setup we need to mkdir and clone the repo and all that stuff.
CONTINUE WITH README SETUP SET
ALSO THINK ABOUT CREATING GLOBAL CONFS FILE FOR ALL NECESSARY VARIABLES!


# airflow is now installed
--need to figure out how dags stuff
--also when closing ssh remote session, airflow webserver ui not work anymore in localhost:8080
---if want to access ui again, need to ssh remote and then run the port forward command again.
---How to solve this??




#DONE:
## create test functions to determine if terraform resources or aws or hetzner resources exist
#And everytime syncing job, need to add pip install --requirements.txt there?
# init github:
--use similar principal as with terraform variables
---save to configuration file and use some payload script
#init_remote:
--save connections while initialisation, not in the functions
--save commands to file and run that file, how to make it work?
# To the initialisation, we need to add logic of creating venv in there?
#create update_repo_secret and update_repo_variables where logic is:
--remove all secrets, and then create all again
# Add s3 resource
# delete legacy git variables, make sure that init_gihtub works, fix main.yml
 -- done
# should the initialise.py be "ManageProject.py" since it's a class and should it be in the setup folder??
 --done, and moved to parent folder
# main.yml should run only when some pull request tag is added to the pull request. How to?
 --done based on latest commit message. If branch latest commit message is NOT "skip syncing to remote", trigger workflow.
 --so if you dont want to trigger the workflow, specify the string as last commit message.
github api and secret and variable create to initialise.py
also modify workflow file once variables are added to github.
-initiliase.py add init resource stuff now that we have remotessh and hetznerapi done.
---problem in init_remote_serve (freezes after cloning repo)
-----Might be that git prompts you a git global stuff which we should just ignore, should we just create configs first before cloning or?

-continue hetznerapi and remotessh classes
- finalise vcp provider stuff in workspace creation (need to define pattern there to when apply runs)
#git workflow
--Need to configure .github/workflows/main.yml file
---steps:
----get runner ip
----give temporary access to remote server for that ip
----do the ssh tricks
----remove the ip from the firewall
----if any errors occur in the workflow we need to remove the ip from the firewall settings still.

# Install airflow:
--Test kubernetes and helm chart installation (when installing kubernetes, check cpu usage of the hetzner server)
----This was not due to kubernetes and helm set up, this was due to vscode setting ~"search.symlink"
# finish documents
# toml file
# change name of the project
# Create TFCloud class and think about the functions and attributes there
# Create docs md of the remote connection
# Create proper key files and make sure to save them in proper folder (do it in separate py file)
# check that you have remote access now
# create manage_tf_cloud.py

#NOTES:
## You need to have correct indentityfile to access remotely in ssh:
---example this creates a connection: ssh root@159.69.35.153 -i .ssh-tf-hetzner-cloud/id_rsa.key
---and then in .ssh config file replace the identity file attribute with correct absolute path